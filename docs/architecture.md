---
layout: default
title: Architecture
---

**Navigation**: [Home](index.md) | [Changelog](changelog.md) | [Architecture](architecture.md) | [GitHub](https://github.com/steve-dickinson/agentic-environmental-intelligence)

# System Architecture

> **âš ï¸ Note**: This is a proof of concept and personal project, not a production system. The architecture described here is for educational and demonstration purposes.

## High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Scheduled Agentic Workflow (LangGraph)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Agent  â”‚â”€â”€â”€â–¶â”‚  Tools   â”‚â”€â”€â”€â–¶â”‚  Analysis   â”‚             â”‚
â”‚  â”‚  LLM   â”‚    â”‚ (MCP)    â”‚    â”‚  Pipeline   â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                â”‚                    â”‚              â”‚
â”‚         â”‚   Every 2 Hours (Docker Scheduler)  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                â”‚                    â”‚
         â–¼                â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI    â”‚  â”‚  EA Data APIs    â”‚  â”‚ Triple Storage   â”‚
â”‚   GPT-4     â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ Flood Monitoring â”‚  â”‚ MongoDB          â”‚
                 â”‚   Hydrology      â”‚  â”‚ (Incidents +     â”‚
                 â”‚   Rainfall       â”‚  â”‚  Agent Logs)     â”‚
                 â”‚ Public Registers â”‚  â”‚                  â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ PostgreSQL       â”‚
                                       â”‚ (pgvector RAG)   â”‚
                                       â”‚                  â”‚
                                       â”‚ Neo4j            â”‚
                                       â”‚ (Knowledge Graph)â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
                                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                       â”‚ Streamlit        â”‚
                                       â”‚ 3-Page Dashboard â”‚
                                       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                       â”‚ 1. Incidents     â”‚
                                       â”‚ 2. Agent Runs    â”‚
                                       â”‚ 3. RAG vs Graph  â”‚
                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Agent Decision Flow

```
START
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Initialize Mission   â”‚
â”‚  - Set system prompt  â”‚
â”‚  - Define workflow    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Agent Thinking      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   (LLM Decision)      â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
  â”‚                                 â”‚
  â”œâ”€ Tool Call? â”€â”€Yesâ”€â”€â–¶ Execute Tools
  â”‚                      (Flood/Hydro/Rainfall/Permits)
  â”‚                                 â”‚
  â”œâ”€ Has Data? â”€â”€Yesâ”€â”€â–¶ Detect Anomalies
  â”‚                      - Threshold check
  â”‚                      - Temporal filter
  â”‚                      - Spatial clustering
  â”‚                                 â”‚
  â”œâ”€ Anomalies? â”€Yesâ”€â”€â–¶ Generate Incidents
  â”‚                      - Per-cluster analysis
  â”‚                      - Rainfall correlation
  â”‚                      - Permit enrichment
  â”‚                      - Context-aware summaries
  â”‚                                 â”‚
  â””â”€ Complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â–¼
END
```

## Data Processing Pipeline

### 1. Collection Phase
```
API Calls (Parallel)
â”œâ”€ Flood Monitoring API â†’ 761 stations
â”œâ”€ Hydrology API        â†’ 1,000 stations
â””â”€ Rainfall API         â†’ 383 stations
                            â”‚
                            â–¼
                  Coordinate Enrichment
                  (MongoDB Station Repo)
                            â”‚
                            â–¼
                  Standardized Readings
                  {station_id, value, timestamp,
                   source, lat, lon, easting, northing}
```

### 2. Detection Phase
```
All Readings (9,000+)
        â”‚
        â–¼
Threshold Filter (configurable)
        â”‚
        â–¼
Anomalies (100-200)
        â”‚
        â–¼
Temporal Filter (24h)
        â”‚
        â–¼
Recent Anomalies (50-100)
        â”‚
        â–¼
Spatial Clustering (10km)
        â”‚
        â–¼
Incident Clusters (3-7)
```

### 3. Enrichment Phase
```
For Each Cluster:
  â”‚
  â”œâ”€ Calculate Center Point
  â”‚   (average lat/lon)
  â”‚
  â”œâ”€ Search Permits (1km radius)
  â”‚   - Discharge consents
  â”‚   - Waste operations
  â”‚   - Flood risk activities
  â”‚
  â”œâ”€ Check Rainfall (10km, 24h)
  â”‚   - Total precipitation
  â”‚   - Max reading
  â”‚   - Station count
  â”‚
  â””â”€ Generate Alert Summary
      - Priority classification
      - Data-driven content
      - Source-aware language
      - Actionable recommendations
```

## Storage Architecture

### MongoDB Collections
```
incidents
â”œâ”€ _id (ObjectId)
â”œâ”€ incident_id (UUID, indexed)
â”œâ”€ content_hash (SHA-256, indexed) â† NEW: Duplicate detection
â”œâ”€ created_at (datetime)
â”œâ”€ readings (array)
â”‚   â””â”€ {station_id, value, timestamp, source, coordinates}
â”œâ”€ alerts (array)
â”‚   â””â”€ {summary, priority, suggested_actions}
â””â”€ permits (array)
    â””â”€ {permit_id, operator, type, distance}

agent_run_logs â† NEW: Execution tracking
â”œâ”€ _id (ObjectId)
â”œâ”€ run_id (UUID, unique index)
â”œâ”€ timestamp (datetime, descending index)
â”œâ”€ duration_seconds (float)
â”œâ”€ stations_fetched (int)
â”œâ”€ readings_fetched (int)
â”œâ”€ flood_warnings_fetched (int)
â”œâ”€ clusters_found (int)
â”œâ”€ cluster_details (array)
â”‚   â””â”€ {type, station_count, station_ids, center_lat, center_lon}
â”œâ”€ rag_searches_performed (int)
â”œâ”€ rag_results (array)
â”‚   â””â”€ {similar_incidents_found, avg_similarity, best_similarity}
â”œâ”€ incidents_created (int, indexed)
â”œâ”€ incidents_duplicate (int)
â”œâ”€ incident_ids_created (array)
â”œâ”€ incident_ids_duplicate (array)
â”œâ”€ mongodb_stored (int)
â”œâ”€ pgvector_stored (int)
â”œâ”€ neo4j_stored (int)
â”œâ”€ errors (array)
â””â”€ openai_api_calls (int)

station_metadata
â”œâ”€ _id (composite: "source:station_id")
â”œâ”€ source (flood|hydrology|rainfall)
â”œâ”€ station_id (string)
â”œâ”€ lat, lon (WGS84)
â”œâ”€ easting, northing (British National Grid)
â”œâ”€ label (string)
â””â”€ last_seen (datetime)
```

### PostgreSQL (pgvector)
```
incident_embeddings
â”œâ”€ id (UUID, primary key)
â”œâ”€ run_id (UUID) â† NEW: Track which run created this
â”œâ”€ embedding (vector(1536))
â”œâ”€ summary_text (text)
â”œâ”€ created_at (timestamp)
â””â”€ UNIQUE CONSTRAINT (id) â† NEW: Prevent duplicates

Indexes:
â”œâ”€ id (unique, for duplicate detection)
â””â”€ embedding (HNSW for cosine similarity search)

Purpose: RAG semantic search and similarity analysis
```

### Neo4j Graph Database â† NEW
```
Nodes:
â”œâ”€ Incident
â”‚   â””â”€ Properties: incident_id, summary, priority, timestamp
â”œâ”€ Station
â”‚   â””â”€ Properties: station_id, label, lat, lon, source
â”œâ”€ Permit
â”‚   â””â”€ Properties: permit_id, operator, type, distance
â””â”€ Location
    â””â”€ Properties: name, lat, lon

Relationships:
â”œâ”€ (Incident)-[:MEASURED_AT]->(Station)
â”œâ”€ (Incident)-[:NEAR_PERMIT]->(Permit)
â”œâ”€ (Station)-[:IN_CATCHMENT]->(Location)
â””â”€ (Incident)-[:SIMILAR_TO]->(Incident)

Current Stats: 77 nodes, 72 relationships
Purpose: Causal reasoning and multi-hop queries
```

### Storage Flow with Duplicate Detection

```
New Incident Generated
        â”‚
        â–¼
    MongoDB Check
    â”œâ”€ Generate content_hash (SHA-256)
    â”œâ”€ Query: existing incident with same hash?
    â”œâ”€ Within 24h window?
    â”‚
    â”œâ”€ YES: Return existing incident (skip storage)
    â”‚         Log: "â„¹ï¸ Duplicate incident detected"
    â”‚
    â””â”€ NO: Continue to storage
        â”‚
        â–¼
    PostgreSQL Check
    â”œâ”€ Query: embeddings exist for incident_id?
    â”‚
    â”œâ”€ YES: Skip embedding generation
    â”‚         (Saves OpenAI API call)
    â”‚
    â””â”€ NO: Generate embedding
        â”‚   Store to pgvector
        â”‚
        â–¼
    Neo4j Check
    â”œâ”€ Query: node exists for incident_id?
    â”‚
    â”œâ”€ YES: Skip graph creation
    â”‚
    â””â”€ NO: Create incident node
            Create station relationships
            Link to permits
            
Result: Idempotent storage across all 3 databases
```

## MCP Tools Pattern

```python
@tool
async def get_flood_readings(parameter: str) -> dict:
    """Fetch latest flood monitoring data."""
    client = FloodClient()
    readings = await client.get_latest_readings(parameter)
    return {"readings": [...], "count": len(readings)}
```

**Available Tools:**
- `get_flood_readings` - River levels, flow rates
- `get_hydrology_readings` - Groundwater, water quality
- `get_rainfall_readings` - Precipitation data
- `search_public_registers` - Environmental permits

## Client Architecture

```
BaseClient Pattern
â”œâ”€ FloodClient
â”‚   â”œâ”€ get_latest_readings()
â”‚   â””â”€ _enrich_coordinates()
â”‚
â”œâ”€ HydrologyClient
â”‚   â”œâ”€ get_latest_readings()
â”‚   â””â”€ _enrich_coordinates()
â”‚
â”œâ”€ RainfallClient
â”‚   â”œâ”€ get_latest_readings()
â”‚   â”œâ”€ get_rainfall_near_location()
â”‚   â”œâ”€ calculate_total_rainfall()
â”‚   â””â”€ _enrich_coordinates()
â”‚
â””â”€ PublicRegistersClient
    â”œâ”€ search_by_coordinates()
    â””â”€ search_by_postcode()
```

## Coordinate Enrichment Strategy

```
Reading from API (minimal data)
  â”‚
  â–¼
Extract station_id
  â”‚
  â–¼
Query MongoDB station_metadata
  â”œâ”€ Try source-specific lookup
  â”œâ”€ Fallback to alternate sources
  â””â”€ Use cached metadata
  â”‚
  â–¼
Enrich Reading Object
  {
    station_id: "3400TH",
    value: 4.55,
    timestamp: "2025-11-22T00:00:00Z",
    source: "flood",
    easting: 361234,      â† Added
    northing: 175890,     â† Added
    lat: 51.4195,         â† Added
    lon: -0.3087          â† Added
  }
```

## Clustering Algorithm

```python
def cluster_anomalies_spatially(
    readings, 
    max_distance_km=10.0,
    min_cluster_size=2
):
    """DBSCAN-like spatial clustering."""
    
    clusters = []
    used = set()
    
    for reading in readings:
        if reading in used:
            continue
            
        cluster = [reading]
        used.add(reading)
        
        # Find nearby readings
        for other in readings:
            if other in used:
                continue
            
            distance = haversine(
                reading.lat, reading.lon,
                other.lat, other.lon
            )
            
            if distance <= max_distance_km:
                cluster.append(other)
                used.add(other)
        
        if len(cluster) >= min_cluster_size:
            clusters.append(cluster)
    
    return clusters
```

## Alert Generation Strategy

```
Cluster Analysis
  â”‚
  â”œâ”€ Determine source(s): flood, hydrology, or mixed
  â”‚
  â”œâ”€ Calculate statistics
  â”‚   â”œâ”€ Max value
  â”‚   â”œâ”€ Average value
  â”‚   â””â”€ Station count
  â”‚
  â”œâ”€ Check rainfall (if flood cluster)
  â”‚   â””â”€ Categorize: heavy/moderate/light/none
  â”‚
  â”œâ”€ Analyze permits
  â”‚   â”œâ”€ Count by type
  â”‚   â””â”€ Categorize activities
  â”‚
  â””â”€ Generate summary
      â”œâ”€ Source-specific language
      â”œâ”€ Data-driven details
      â”œâ”€ Rainfall context (if applicable)
      â”œâ”€ Permit information
      â””â”€ Actionable recommendations
```

## Deployment Architecture

```
Development & Production (Docker Compose)
  â”‚
  â”œâ”€ Core Infrastructure
  â”‚   â”œâ”€ MongoDB (port 27017)
  â”‚   â”‚   â”œâ”€ incidents collection
  â”‚   â”‚   â”œâ”€ agent_run_logs collection
  â”‚   â”‚   â””â”€ station_metadata collection
  â”‚   â”‚
  â”‚   â”œâ”€ PostgreSQL (port 5432)
  â”‚   â”‚   â”œâ”€ pgvector extension
  â”‚   â”‚   â””â”€ incident_embeddings table
  â”‚   â”‚
  â”‚   â”œâ”€ Neo4j (port 7474, 7687)
  â”‚   â”‚   â”œâ”€ Incident nodes
  â”‚   â”‚   â”œâ”€ Station nodes
  â”‚   â”‚   â””â”€ Relationships
  â”‚   â”‚
  â”‚   â””â”€ pgAdmin (port 5050)
  â”‚
  â”œâ”€ Scheduled Agent Execution â† NEW
  â”‚   â”œâ”€ Docker service: agent
  â”‚   â”œâ”€ Command: infinite loop with 7200s sleep
  â”‚   â”œâ”€ Restart policy: unless-stopped
  â”‚   â”œâ”€ Environment: RUN_INTERVAL_HOURS=2
  â”‚   â””â”€ Runs: Every 2 hours continuously
  â”‚
  â”œâ”€ Python Environment (uv)
  â”‚   â”œâ”€ Dependencies via pyproject.toml
  â”‚   â””â”€ UV package manager
  â”‚
  â”œâ”€ Execution Scripts
  â”‚   â”œâ”€ scripts/run_agent.py (single run)
  â”‚   â”œâ”€ scripts/view_run_logs.py (statistics)
  â”‚   â””â”€ scripts/sync_stations.py (metadata)
  â”‚
  â””â”€ Streamlit Dashboard (port 8501)
      â”œâ”€ Page 1: Incident Dashboard
      â”œâ”€ Page 2: Agent Runs (analytics)
      â””â”€ Page 3: RAG vs Knowledge Graph

Scheduled Execution Flow
  â”‚
  â–¼
[Agent Container Starts]
  â”‚
  â””â”€ while true; do
      â”‚
      â”œâ”€ Generate unique run_id (UUID)
      â”œâ”€ Track start timestamp
      â”‚
      â”œâ”€ Execute: uv run python scripts/run_agent.py
      â”‚   â”‚
      â”‚   â”œâ”€ Fetch data from EA APIs
      â”‚   â”œâ”€ Detect anomalies
      â”‚   â”œâ”€ Cluster spatially
      â”‚   â”œâ”€ Search permits
      â”‚   â”œâ”€ Correlate rainfall
      â”‚   â”œâ”€ Generate incidents
      â”‚   â”œâ”€ RAG enrichment
      â”‚   â”œâ”€ Store to 3 databases (with duplicate detection)
      â”‚   â””â”€ Build AgentRunLog
      â”‚
      â”œâ”€ Save run log to MongoDB
      â”‚   â””â”€ agent_run_logs collection
      â”‚
      â”œâ”€ Print summary:
      â”‚   â±ï¸ Duration: 145.3s
      â”‚   ğŸ“Š Readings: 8,247
      â”‚   ğŸ—ºï¸ Clusters: 3
      â”‚   ğŸ“ Incidents: 5 (2 new, 3 duplicate)
      â”‚   ğŸ” RAG: 2 searches (88% avg similarity)
      â”‚
      â”œâ”€ sleep 7200 (2 hours)
      â”‚
      â””â”€ [Loop repeats]

Monitoring Commands
  â”‚
  â”œâ”€ docker-compose logs -f agent
  â”‚   â””â”€ Real-time execution monitoring
  â”‚
  â”œâ”€ uv run python scripts/view_run_logs.py
  â”‚   â””â”€ Aggregate statistics (7-day default)
  â”‚
  â””â”€ uv run streamlit run streamlit_app.py
      â””â”€ Interactive dashboard with Agent Runs page
```

### Production Considerations (Future)

```
Current: Single Docker Host
  â”‚
Future: Cloud Infrastructure
  â”‚
  â”œâ”€ Managed Databases
  â”‚   â”œâ”€ MongoDB Atlas (incidents + logs)
  â”‚   â”œâ”€ Cloud SQL PostgreSQL (pgvector)
  â”‚   â””â”€ Neo4j Aura (knowledge graph)
  â”‚
  â”œâ”€ Container Orchestration
  â”‚   â”œâ”€ Kubernetes for agent scheduling
  â”‚   â”œâ”€ Horizontal scaling for parallel processing
  â”‚   â””â”€ Service mesh for reliability
  â”‚
  â”œâ”€ Observability
  â”‚   â”œâ”€ Prometheus metrics
  â”‚   â”œâ”€ Grafana dashboards
  â”‚   â””â”€ Structured logging (ELK stack)
  â”‚
  â””â”€ API Gateway
      â”œâ”€ REST API for external access
      â”œâ”€ Webhook alerts to stakeholders
      â””â”€ Authentication/authorization
```

## Performance Optimizations

### 1. Batch Database Queries
```python
# Bad: N queries
for station_id in station_ids:
    metadata = repo.get_station(source, station_id)

# Good: 1 query
metadata_map = repo.bulk_get_stations(source, station_ids)
```

### 2. Parallel Tool Execution
```python
# LangGraph automatically parallelizes independent tool calls
response = await agent.invoke({"messages": [message]})
# Flood + Hydro + Rainfall called simultaneously
```

### 3. Smart Caching
```python
class RainfallClient:
    def __init__(self):
        self._metadata_cache = {}  # Avoid re-fetching
```

### 4. Message Reduction
```python
def reduce_messages(left, right):
    """Trim large ToolMessage content to summaries."""
    # Prevents token bloat in agent context
```

---

[Back to Home](index.md)
